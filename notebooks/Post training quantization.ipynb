{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300e8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3d4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9b80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b5962b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca58d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import latexify\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2a108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a35945c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782da5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root='../data/mnist/train/', train=True, download=True, transform=ToTensor())\n",
    "testing_data = datasets.MNIST(root='../data/mnist/test/', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a6cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0eedcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9dd6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad91efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2ba249",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4ecd0",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1be76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be517501",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c6cb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = nn.MaxPool2d(2, 2)(x)\n",
    "        x = nn.Dropout(0.1)(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = nn.MaxPool2d(2, 2)(x)\n",
    "        x = nn.Dropout(0.1)(x)\n",
    "        \n",
    "        x = rearrange(x, 'b c h d -> b (c h d)')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a277ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 28, 28)*200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8dea64",
   "metadata": {},
   "source": [
    "- Cross enttropy loss in pytorch combines both log_softmax and nll_loss in pytorch, we can either use cross entropy loss and pass it raw logits as input or apply log_softmax on logits and use nll_loss \n",
    "- nll_loss is nothing but multiplication with -1. \n",
    "- Cross entropy loss is -yi*log(p(yi/x)) and since log_softmax has done most of the work only -1 multiplication is remaining\n",
    "- softmax is not used as activation directly since it is numerically unstable while log_softmax is stable\n",
    "- always log_softmax is computed and not softmax it is only used for showing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce5c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{crossentropy}(yi)\\triangleq -yi\\log{\\left({\\mathrm{p}\\left(yi\\right)}\\right)} $$"
      ],
      "text/plain": [
       "<latexify.core.with_latex.<locals>._LatexifiedFunction at 0x7fd2593512b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@latexify.with_latex\n",
    "def crossentropy(yi):\n",
    "    return -yi*math.log(p(yi))\n",
    "\n",
    "crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "371c3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd2dc0b7e30>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "823f5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7e439ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4955a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for batch_idx, (img, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "            out = model(img)\n",
    "            loss = F.nll_loss(out, label) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f'''Train Epoch: {i+1} [{batch_idx*batch_size}/{len(training_data)}'''\n",
    "              f''' ({100*batch_idx*batch_size/len(training_data):.0f}%)]\\tLoss: {loss.item():.6f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8d4b8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(img)\n",
    "            final_loss += F.nll_loss(out, label)\n",
    "            total_correct += (torch.argmax(out, dim=-1) == label).sum().item()\n",
    "\n",
    "    print(f'''\\nTest set: Average loss: {final_loss/len(test_dataloader):.4f},'''\n",
    "          f''' Accuracy: {total_correct}/{len(testing_data)} ({100*total_correct/len(testing_data):.0f})%\\n''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "42eb2346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.93it/s]\n",
      "  1%|          | 5/938 [00:00<00:20, 46.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 2.212657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 42.81it/s]\n",
      "  0%|          | 4/938 [00:00<00:25, 37.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 2.046324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 44.77it/s]\n",
      "  1%|          | 5/938 [00:00<00:20, 45.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 1.100219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 46.02it/s]\n",
      "  1%|          | 5/938 [00:00<00:21, 44.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.889900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:25<00:00, 36.88it/s]\n",
      "  0%|          | 4/938 [00:00<00:23, 39.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.491591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:29<00:00, 32.09it/s]\n",
      "  0%|          | 4/938 [00:00<00:25, 36.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.666147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:22<00:00, 41.47it/s]\n",
      "  0%|          | 4/938 [00:00<00:27, 34.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.379765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:28<00:00, 32.97it/s]\n",
      "  0%|          | 4/938 [00:00<00:25, 36.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.242100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:26<00:00, 34.92it/s]\n",
      "  1%|          | 5/938 [00:00<00:22, 41.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.214043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:22<00:00, 41.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.293046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2dbd7247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3322, Accuracy: 9021/10000 (90)%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3b93e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../saved_models/simple_pytorch_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e39ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "21434e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "    max_val, _ = torch.max(x, dim=1)\n",
    "    min_val, _ = torch.min(x, dim=1)\n",
    "\n",
    "    # add ema calculation\n",
    "    if key not in stats:\n",
    "        stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": x.shape[0]}\n",
    "    else:\n",
    "        stats[key]['max'] += max_val.sum().item()\n",
    "        stats[key]['min'] += min_val.sum().item()\n",
    "        stats[key]['total'] += x.shape[0]\n",
    "\n",
    "    weighting = (x.shape[0] + 1.0) / (stats[key]['total'] + 1)\n",
    "\n",
    "    if 'ema_min' in stats[key]:\n",
    "        stats[key]['ema_min'] = weighting*(min_val.mean().item()) + (1- weighting) * stats[key]['ema_min']\n",
    "    else:\n",
    "        stats[key]['ema_min'] = weighting*(min_val.mean().item())\n",
    "\n",
    "    if 'ema_max' in stats[key]:\n",
    "        stats[key]['ema_max'] = weighting*(max_val.mean().item()) + (1- weighting) * stats[key]['ema_max']\n",
    "    else: \n",
    "        stats[key]['ema_max'] = weighting*(max_val.mean().item())\n",
    "\n",
    "    stats[key]['min_val'] = stats[key]['min']/ stats[key]['total']\n",
    "    stats[key]['max_val'] = stats[key]['max']/ stats[key]['total']\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "    x = F.relu(model.conv1(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    \n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "    x = F.relu(model.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = x.view(-1, 4*4*50)\n",
    "\n",
    "    stats = updateStats(x, stats, 'fc1')\n",
    "    x = F.relu(model.fc1(x))\n",
    "    \n",
    "    stats = updateStats(x, stats, 'fc2')\n",
    "    x = model.fc2(x)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader, device):\n",
    "\n",
    "    model.eval()\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "\n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "        final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "    return final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "221e0964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv1': {'max': tensor(0.9996), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 0.9995996076035778}, 'conv2': {'max': tensor(2.7292), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 2.729194272986898}, 'fc1': {'max': tensor(5.8366), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 5.836630433610367}, 'fc2': {'max': tensor(4.8296), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 4.8297046495287965}}\n"
     ]
    }
   ],
   "source": [
    "stats = gatherStats(model, test_dataloader, device)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111edd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
    "    '''\n",
    "    Calc Scale and zero point of next\n",
    "    Args:\n",
    "        min_val:\n",
    "        max_val:\n",
    "        \n",
    "    Return:\n",
    "        scale:\n",
    "        zero_point:\n",
    "    ''' \n",
    "    \n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "\n",
    "    return scale, zero_point\n",
    "\n",
    "\n",
    "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
    "    '''\n",
    "    Quantise a given tensor\n",
    "    \n",
    "    Args:\n",
    "        x (torch.tensor):\n",
    "        num_bits (int):\n",
    "        min_val (float):\n",
    "        max_val (float):\n",
    "        \n",
    "    Return:\n",
    "        Quantized tensor\n",
    "    '''\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "        min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "\n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    '''\n",
    "    Dequantize given tensor\n",
    "    \n",
    "    Args:\n",
    "        q_x (quantised tensor):\n",
    "    \n",
    "    Returns:\n",
    "        dequantized tensor\n",
    "    \n",
    "    '''\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "a0b8c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "    \n",
    "class QuantNet(nn.Module):\n",
    "    \n",
    "    QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "    \n",
    "    def __init__(self, stats=None, scale_zp={}, num_bits=8):\n",
    "        super(QuantNet, self).__init__()\n",
    "        \n",
    "        self.stats = stats  \n",
    "        \n",
    "        # int layers\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 100)\n",
    "        # float layer\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.scale_zp = scale_zp\n",
    "\n",
    "    \n",
    "    def calcScaleZeroPoint(self, min_val, max_val, num_bits=8):\n",
    "        '''\n",
    "        Calc Scale and zero point of next\n",
    "        Args:\n",
    "            min_val:\n",
    "            max_val:\n",
    "\n",
    "        Return:\n",
    "            scale:\n",
    "            zero_point:\n",
    "        ''' \n",
    "\n",
    "        qmin = 0.\n",
    "        qmax = 2.**num_bits - 1.\n",
    "\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "        initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "        zero_point = 0\n",
    "        if initial_zero_point < qmin:\n",
    "            zero_point = qmin\n",
    "        elif initial_zero_point > qmax:\n",
    "            zero_point = qmax\n",
    "        else:\n",
    "            zero_point = initial_zero_point\n",
    "\n",
    "        zero_point = int(zero_point)\n",
    "\n",
    "        return scale, zero_point\n",
    "\n",
    "\n",
    "    def quantize_tensor(self, x, num_bits=8, min_val=None, max_val=None):\n",
    "        '''\n",
    "        Quantise a given tensor\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor):\n",
    "            num_bits (int):\n",
    "            min_val (float):\n",
    "            max_val (float):\n",
    "\n",
    "        Return:\n",
    "            Quantized tensor\n",
    "        '''\n",
    "\n",
    "        if not min_val and not max_val: \n",
    "            min_val, max_val = x.min(), x.max()\n",
    "\n",
    "        qmin = 0.\n",
    "        qmax = 2.**num_bits - 1.\n",
    "\n",
    "        scale, zero_point = self.calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "        q_x = zero_point + x / scale\n",
    "        q_x.clamp_(qmin, qmax).round_()\n",
    "        q_x = q_x.round().byte()\n",
    "\n",
    "        return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "\n",
    "    def dequantize_tensor(self, q_x):\n",
    "        '''\n",
    "        Dequantize given tensor\n",
    "\n",
    "        Args:\n",
    "            q_x (quantised tensor):\n",
    "\n",
    "        Returns:\n",
    "            dequantized tensor\n",
    "\n",
    "        '''\n",
    "        return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "    \n",
    "    \n",
    "    def quantizeLayer(self, x, layer, key, stat, scale_x, zp_x):\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.tensor):\n",
    "            layer:\n",
    "            key:\n",
    "            stat:\n",
    "            scale_x:\n",
    "            zp_x:\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            x, scale_next, zero_point_next\n",
    "\n",
    "        '''\n",
    "        w = layer.weight.data\n",
    "        b = layer.bias.data\n",
    "        \n",
    "        scale_w = self.scale_zp[key]['scale_w']\n",
    "        zp_w = self.scale_zp[key]['zp_w']\n",
    "        scale_b = self.scale_zp[key]['scale_b']\n",
    "        zp_b = self.scale_zp[key]['zp_b']\n",
    "\n",
    "        scale_next, zero_point_next = self.calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "        # Preparing input by saturating range to num_bits range.\n",
    "        X = x.float() - zp_x\n",
    "        \n",
    "        # conversion of weight and bias to float is done since they are in uint8 range which is limited to 0, 255 \n",
    "        # and thus if subtraction or multiplication leads to \n",
    "        # number outside its range it will cause issue since that number will be converted into 0, 255 range\n",
    "        layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data.float() - zp_w)\n",
    "        layer.bias.data = (scale_b/scale_next)*(layer.bias.data.float() + zp_b)\n",
    "\n",
    "        # All int computation\n",
    "        x = (layer(X)) + zero_point_next \n",
    "\n",
    "        # cast to int\n",
    "        x.round_()\n",
    "        \n",
    "        layer.weight.data = w\n",
    "        layer.bias.data = b\n",
    "\n",
    "        return x, scale_next, zero_point_next\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.quantize_tensor(x, min_val=self.stats['conv1']['min'], max_val=self.stats['conv1']['max'])\n",
    "        \n",
    "        \n",
    "        x, scale_next, zero_point_next = self.quantizeLayer(x=x.tensor, layer=self.conv1, key='conv1', stat=stats['conv2'], \n",
    "                                                       scale_x = x.scale, zp_x = x.zero_point)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x, scale_next, zero_point_next = self.quantizeLayer(x=x, layer=self.conv2, key='conv2', stat=stats['fc1'], \n",
    "                                                       scale_x = scale_next, zp_x = zero_point_next)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "#         x = x.view(-1, 4*4*50)\n",
    "        x = rearrange(x, 'b c h d -> b (c h d)')\n",
    "\n",
    "        x, scale_next, zero_point_next = self.quantizeLayer(x=x, layer=self.fc1, key='fc1', stat=stats['fc2'], \n",
    "                                                       scale_x = scale_next, zp_x = zero_point_next)\n",
    "        \n",
    "        # Back to dequant for final layer\n",
    "        x = self.dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "68b635f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = QuantNet(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "4e12dbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd644c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "20eed2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_layer(scale_zp, layer, key):\n",
    "    scale_zp[key] = {}\n",
    "    qtensor_weight = quantize_tensor(layer.weight.data)\n",
    "    layer.weight.data, scale_zp[key]['scale_w'], scale_zp[key]['zp_w'] = \\\n",
    "        deepcopy(qtensor_weight.tensor), qtensor_weight.scale, qtensor_weight.zero_point\n",
    "    \n",
    "    qtensor_bias = quantize_tensor(layer.bias.data)\n",
    "    layer.bias.data, scale_zp[key]['scale_b'], scale_zp[key]['zp_b'] = \\\n",
    "        deepcopy(qtensor_bias.tensor), qtensor_bias.scale, qtensor_bias.zero_point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "83588915",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_layer(q_model.scale_zp, q_model.conv1, 'conv1')\n",
    "quantize_layer(q_model.scale_zp, q_model.conv2, 'conv2')\n",
    "quantize_layer(q_model.scale_zp, q_model.fc1, 'fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b59da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "71d7d69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantNet(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebdfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "d042498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3342, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3339, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3358, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3344, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3344, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3349, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3340, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3348, Accuracy: 9062/10000 (91)%\n",
      "\n",
      "1.8 s ± 36.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f569d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3266, Accuracy: 9028/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3329, Accuracy: 9052/10000 (91)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3289, Accuracy: 9025/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3307, Accuracy: 9030/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3298, Accuracy: 9011/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3266, Accuracy: 9012/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3296, Accuracy: 9005/10000 (90)%\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3312, Accuracy: 9015/10000 (90)%\n",
      "\n",
      "1.65 s ± 27.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9f880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "591b23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_model.state_dict(), '../saved_models/post_training_quantized_pytorch_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'state_dict': q_model.state_dict(),\n",
    "    'stats': stats,\n",
    "    'scale_zp': q_model.scale_zp\n",
    "    }\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../saved_models/quantize.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1455df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../saved_models/quantize.pickle', 'rb') as handle:\n",
    "    model_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2bcc59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = QuantNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4a21348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.load_state_dict(model_loaded['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cd8047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.stats = model_loaded['stats']\n",
    "model_new.scale_zp = model_loaded['scale_zp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d70d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env2",
   "language": "python",
   "name": "conda_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
